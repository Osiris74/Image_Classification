{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f050ded6",
   "metadata": {},
   "source": [
    "Задание классификации изображений героев сериала симпсоны\n",
    "\n",
    "\n",
    "Первым делом подключим все необходимые библиотеки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2133add7",
   "metadata": {},
   "source": [
    "Kaggle contest: https://www.kaggle.com/competitions/journey-springfield/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c34ed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If using colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')\n",
    "\n",
    "!unzip -q /content/gdrive/My\\ Drive/journey-springfield.zip\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0242b209",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking through the dir\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18f3db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6102025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There were troubles with Pillow\n",
    "#Now it's working fine\n",
    "#If not - uncomment the \"!pip install Pillow\" line \n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#!pip uninstall -y Pillow\n",
    "#!pip install Pillow\n",
    "\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "#from skimage import io\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb868656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c01dcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working with Cuda\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "\n",
    "#Saving dirs with data\n",
    "TRAIN_DIR = Path('train/simpsons_dataset')\n",
    "TEST_DIR = Path('testset/testset')\n",
    "\n",
    "#Here we do not use validation, to save images for better training\n",
    "train_files = sorted(list(TRAIN_DIR.rglob('*.jpg')))\n",
    "train_labels = [path.parent.name for path in train_files]\n",
    "test_files = sorted(list(TEST_DIR.rglob('*.jpg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4493d2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking through the data\n",
    "data = pd.Series(train_labels).value_counts().sort_values().to_frame(\"count\")\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This was used to try to solve the problem with NUM_WORKERS \n",
    "\n",
    "\n",
    "#class_count = {}\n",
    "#for i in range(len(data)):\n",
    "#    class_count[data.iloc[i].name] = data.iloc[i].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf5887d",
   "metadata": {},
   "source": [
    "Датасет слишком маленький => необходимо применить аугментацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d9f5eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739bc9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "RESCALE_SIZE = 299\n",
    "#The value bigger than 0 could not work on Win (try to solve it)\n",
    "N_CORES = 0\n",
    "DEVICE = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d972c239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Augmentations here\n",
    "#Resizing the image, making the Horizontal flip, and invert\n",
    "#After that we are forming tensor\n",
    "\n",
    "augmentations = transforms.RandomChoice([\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=299, max_size=300),\n",
    "        transforms.CenterCrop(size=299),\n",
    "        #transforms.RandomCrop(250)\n",
    "    ]),\n",
    "    transforms.RandomRotation(degrees=(-25, 25)),\n",
    "    transforms.RandomHorizontalFlip(p=1),\n",
    "    transforms.RandomInvert(p=0.3)\n",
    "])\n",
    "train_transforms = transforms.Compose([\n",
    "    augmentations,\n",
    "    transforms.Resize(size=(RESCALE_SIZE, RESCALE_SIZE)),\n",
    "    transforms.Lambda(lambda x: np.array(x, dtype=\"float32\") / 255),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ,\n",
    "])\n",
    "train_dataset = datasets.ImageFolder(root=TRAIN_DIR, transform=train_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96909bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dfa54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we creating different weights for all classes\n",
    "#For classes with small number of elements weights are bigger\n",
    "\n",
    "class_count = Counter(train_dataset.targets)\n",
    "class_weights = {i: 1/c for i, c in class_count.items()}\n",
    "sample_weights = [0] * len(train_dataset)\n",
    "for i, (data, label) in enumerate(tqdm(train_dataset)):\n",
    "    class_weight = class_weights[label]\n",
    "    sample_weights[i] = class_weight\n",
    "\n",
    "#Creating train sampler with different weights\n",
    "N = max(class_count.values()) * len(class_count)  # fit to max\n",
    "train_sampler = WeightedRandomSampler(sample_weights, num_samples=N, replacement=True)\n",
    "\n",
    "#Train loader was moved to train method to save the GPU memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=N_CORES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aa2440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1ef5527",
   "metadata": {},
   "source": [
    "Let's look on the pictures inside the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4e54da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to show images from tensor representation\n",
    "def imshow(inp, title=None, plt_ax=plt, default=False):\n",
    "    \"\"\"Imshow для тензоров\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt_ax.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt_ax.set_title(title)\n",
    "    plt_ax.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cb240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = plt.subplots(nrows=3, ncols=4, figsize=(12, 9))[1]\n",
    "data, labels = next(iter(train_loader))\n",
    "for ax, d, l in zip(axes.flatten(), data, labels):\n",
    "    imshow(d.data.cpu(), title=train_dataset.classes[l], plt_ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36404cf7",
   "metadata": {},
   "source": [
    "Let's build the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b3f0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#My variant of CNN for image classification\n",
    "\n",
    "#I am using 5 layers of batchNorm to normalize data\n",
    "#I am using two Sequential layers (like in AlexNet) to increase the chance for image classification\n",
    "\n",
    "#It is not good to use more than 16 layers\n",
    "\n",
    "class SimpleCnn(nn.Module):\n",
    "  \n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv5 =nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv6 = nn.Flatten()\n",
    "\n",
    "        self.conv8 = nn.Sequential(\n",
    "            nn.Linear(in_features=3200, out_features=8192, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv9 = nn.Sequential(\n",
    "            nn.Linear(in_features=8192, out_features=4096, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.out = nn.Linear(in_features=4096, out_features=n_classes, bias=True)\n",
    "  \n",
    "  \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.conv6(x)\n",
    "        #x = self.conv7(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.conv9(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.out(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2180c78e",
   "metadata": {},
   "source": [
    "Then we will introduce training course of NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850834a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c634d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_epoch(model, train_loader, criterion, optimizer, scheduler):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_data = 0\n",
    "  \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = torch.argmax(outputs, 1)\n",
    "        running_loss += (loss.item() * inputs.size(0))\n",
    "        running_corrects += (torch.sum(preds == labels.data))\n",
    "        processed_data += (inputs.size(0))\n",
    "        \n",
    "#This part was added to reduce GPU memory usage        \n",
    "        del inputs\n",
    "        del labels\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    scheduler.step()          \n",
    "    train_loss = (running_loss / processed_data)\n",
    "    train_acc = (running_corrects.cpu().numpy() / processed_data)\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d40efc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5626ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_files, model, loss, optimizer, scheduler, epochs, batch_size):\n",
    "   \n",
    "    #Saving data to the memory here to reduce GPU memory usage\n",
    "    train_loader = DataLoader(train_files, batch_size=batch_size, sampler=train_sampler, num_workers=N_CORES)\n",
    "\n",
    "    history = []\n",
    "    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} train_acc {t_acc:0.4f}\"\n",
    "\n",
    "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = fit_epoch(model, train_loader, criterion, opt, scheduler)\n",
    "            history.append((train_loss, train_acc))\n",
    "            print(\"loss\", train_loss)\n",
    "            \n",
    "            history.append((train_loss, train_acc))\n",
    "            \n",
    "            pbar_outer.update(1)\n",
    "            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss, t_acc=train_acc))\n",
    "            \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e481089a",
   "metadata": {},
   "source": [
    "Function to predict the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ff4efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        logits = []\n",
    "    \n",
    "        for inputs in test_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            model.eval()\n",
    "            outputs = model(inputs).cpu()\n",
    "            logits.append(outputs)\n",
    "            \n",
    "    probs = F.softmax(torch.cat(logits), dim=-1).numpy()\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779301c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(train_labels))\n",
    "simple_cnn = SimpleCnn(n_classes).to(DEVICE)\n",
    "print(\"we will classify :{}\".format(n_classes))\n",
    "print(simple_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3213378e",
   "metadata": {},
   "source": [
    "Задаем оптимайзер, функцию потери и шедулер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1e9b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.AdamW(simple_cnn.parameters(), lr=1e-4, amsgrad=True, betas=(0.9, 0.99), weight_decay=0.03)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer=opt, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e80d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training our model and saving loss information\n",
    "history = train(model=simple_cnn, loss=loss, optimizer=opt, scheduler=scheduler, epochs=10, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebe0557",
   "metadata": {},
   "source": [
    "Строим кривые обучения моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf8e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss_plot(loss):\n",
    "    plt.figure(figsize=(15, 9))\n",
    "    plt.plot(loss, label=\"train_loss\")\n",
    "    #plt.plot(val_loss, label=\"val_loss\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b554cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = zip(*history)\n",
    "build_loss_plot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd50329",
   "metadata": {},
   "source": [
    "Метрика f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c2ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(actual_labels, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c405a483",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c459ff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating test dataset to prove the skills of the model\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, files):\n",
    "        self.files = files\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        test_transforms = transforms.Compose([\n",
    "            transforms.Resize(size=(RESCALE_SIZE, RESCALE_SIZE)),\n",
    "            transforms.Lambda(lambda x: np.array(x, dtype=\"float32\") / 255),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ,\n",
    "        ])\n",
    "        img = Image.open(self.files[index]).convert('RGB')\n",
    "        return test_transforms(img)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0005981b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7c1750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_for_test(model):\n",
    "    test_dataset = TestDataset(test_files)\n",
    "    test_loader = DataLoader(test_dataset, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "    #Making predictions about images\n",
    "    probs = predict(model, test_loader)\n",
    "    preds = [train_dataset.classes[i] for i in probs.argmax(axis=-1)]\n",
    "\n",
    "    test_filenames = [path.name for path in test_dataset.files]\n",
    "    \n",
    "    return test_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c0869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filenames = create_dataset_for_test(simple_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076cb223",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forming the result .csv file\n",
    "pd.DataFrame({\n",
    "    \"Id\": test_filenames,\n",
    "    \"Expected\": preds\n",
    "}).to_csv(\"MyCNN_aug.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd5f85b",
   "metadata": {},
   "source": [
    "Score on kaggle platform: 0.99893"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df765af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7bec4af",
   "metadata": {},
   "source": [
    "Картиночки для отладки работы сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ea970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3, ncols=3,figsize=(12, 12), \\\n",
    "                        sharey=True, sharex=True)\n",
    "for fig_x in ax.flatten():\n",
    "    random_characters = int(np.random.uniform(0,1000))\n",
    "    im_val, label = train_dataset[random_characters]\n",
    "    img_label = \" \".join(map(lambda x: x.capitalize(),\\\n",
    "                train_dataset.label_encoder.inverse_transform([label])[0].split('_')))\n",
    "    \n",
    "    \n",
    "\n",
    "    imshow(im_val.data.cpu(), \\\n",
    "          title=img_label,plt_ax=fig_x)\n",
    "    \n",
    "    actual_text = \"Actual : {}\".format(img_label)\n",
    "            \n",
    "    fig_x.add_patch(patches.Rectangle((0, 53),86,35,color='white'))\n",
    "    font0 = FontProperties()\n",
    "    font = font0.copy()\n",
    "    font.set_family(\"fantasy\")\n",
    "    prob_pred = predict_one_sample(simple_cnn, im_val.unsqueeze(0))\n",
    "    predicted_proba = np.max(prob_pred)*100\n",
    "    y_pred = np.argmax(prob_pred)\n",
    "    \n",
    "    predicted_label = label_encoder.classes_[y_pred]\n",
    "    predicted_label = predicted_label[:len(predicted_label)//2] + '\\n' + predicted_label[len(predicted_label)//2:]\n",
    "    predicted_text = \"{} : {:.0f}%\".format(predicted_label,predicted_proba)\n",
    "            \n",
    "    fig_x.text(1, 59, predicted_text + ' here ' , horizontalalignment='left', fontproperties=font,\n",
    "                    verticalalignment='top',fontsize=8, color='black',fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691e6aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521b0b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d201382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the VGG 16 model\n",
    "model_extractor = models.vgg16(pretrained=True)\n",
    "\n",
    "#Creating 4 unfreezed layers\n",
    "layers_to_unfreeze = 4\n",
    "\n",
    "#Freezing layers according to the statement\n",
    "for param in model_extractor.features[:-layers_to_unfreeze].parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# num_features -- features size, going to the FC-layer (look through the model to understand)\n",
    "num_features = 25088\n",
    "n_classes = len(np.unique(train_labels))\n",
    "#Changing fully-connected layer to ours to classificate our images\n",
    "model_extractor.classifier = nn.Linear(num_features, n_classes)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model_extractor = model_extractor.to(DEVICE)\n",
    "\n",
    "#Forming loss, optimizer, criterion an scheduler\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.AdamW(model_extractor.parameters(), lr=1e-4, amsgrad=True, betas=(0.9, 0.99), weight_decay=0.03)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer=opt, step_size=4, gamma=0.1)\n",
    "\n",
    "print(model_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a840012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6083fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clearing the cache\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b17903",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training our model and saving the loss\n",
    "history_vgg16 = train(train_dataset, model=model_extractor, loss=loss, optimizer=opt, scheduler=scheduler, \n",
    "                      epochs=20, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d25745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e299a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filenames = create_dataset_for_test(model_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158ddc09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4102e36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the predictions\n",
    "pd.DataFrame({\n",
    "    \"Id\": test_filenames,\n",
    "    \"Expected\": preds\n",
    "}).to_csv(\"transferLr_aug.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69042625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdb4a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_vgg16, acc_vgg16 = zip(*history_vgg16)\n",
    "build_loss_plot(loss_vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecfb31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "torch.save(model_extractor.state_dict(), 'VGG16.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dc4256",
   "metadata": {},
   "source": [
    "Score on kaggle platform: 0.97874"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d605c6a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e1944c1",
   "metadata": {},
   "source": [
    "Let's try inception v_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4f329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting model \n",
    "model_inception = models.inception_v3(pretrained=True)\n",
    "print(model_inception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083e90a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(train_labels))\n",
    "\n",
    "#Forming two layers of the network to classificate my images\n",
    "model_inception.AuxLogits.fc = nn.Linear(768, n_classes)\n",
    "model_inception.fc = nn.Linear(2048, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6152d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# num_features -- features size, going to the FC-layer (look through the model to understand)\n",
    "num_features = 25088\n",
    "\n",
    "#Changing fully connected layer to my own layer\n",
    "model_inception.classifier = nn.Linear(num_features, n_classes)\n",
    "\n",
    "#Using GPU\n",
    "model_inception = model_inception.to(DEVICE)\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.AdamW(model_inception.parameters(), lr=1e-4, amsgrad=True, betas=(0.9, 0.99), weight_decay=0.03)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer=opt, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1127466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_inception.aux_logits = False\n",
    "\n",
    "#Training model and saving the loss \n",
    "history_inception = train(train_dataset, model=model_inception, loss=loss_fn, optimizer=optimizer, scheduler=exp_lr_scheduler, \n",
    "                          epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267119b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filenames = create_dataset_for_test(model_inception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0010e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    \"Id\": test_filenames,\n",
    "    \"Expected\": preds\n",
    "}).to_csv(\"inceptionModel_aug.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45740e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_inception, acc_inception = zip(*history_inception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e015a9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_loss_plot(loss_inception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7068f90f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88520f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "torch.save(model_inception.state_dict(), 'Inception.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f26d2db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
